{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo 6.2 - Word2Vec\n",
    "In this brief demo, we'll examine how we might develop word embeddings from some custom corpus. Specifically, we'll use data from StockTwits to build our embeddings. \n",
    "\n",
    "We'll do this in three parts: \n",
    "1. Load & clean the data\n",
    "2. Train the Word2Vec model\n",
    "3. Examine language similarity\n",
    "\n",
    "### Step 1 - Load & Clean the Data\n",
    "The data I've provided is a set of tweets about GameStop (GME). Ideally you'd use a large sample of tweets that covers a more than a single stock, but the StockTwits data is massive, so it was easiest to simply analyze a subset. GME investors also use \"colorful\" language which provides a nice opportunity to learn some nuances.\n",
    "\n",
    "Let's load the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88c76e3114e445de"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "folder = \"/Users/cooperdenning/PycharmProjects/pythonProject5/.venv/D6.2\" # update with location to your files\n",
    "filepath = f\"{folder}/GMEstockTwitsV2_sample.csv.gz\"\n",
    "df = pd.read_csv(filepath)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:34:59.562911Z",
     "start_time": "2024-11-11T19:34:58.928988Z"
    }
   },
   "id": "7492eab309afc184",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          twitID          author  author_followers  author_ideas author_join  \\\n",
       "0      278552853      Investeren                54          3790  2012-10-22   \n",
       "1      277914988  flappinAnolips                 9          1554  2020-04-22   \n",
       "2      286576565          jag2pr                10           287  2018-05-16   \n",
       "3      299825525       Tavrabbit                 1           328  2020-12-08   \n",
       "4      276427744       Pupperzzz                 3           100  2015-12-28   \n",
       "...          ...             ...               ...           ...         ...   \n",
       "99995  291526781          PLelek               519          6407  2016-05-25   \n",
       "99996  272968454          kluski                 4            80  2016-03-11   \n",
       "99997  294751108           Cheze                13           703  2020-08-01   \n",
       "99998  277303198   PhatPuffDaddy               198         14351  2019-04-29   \n",
       "99999  286646208        3monkeys                12          2362  2020-11-23   \n",
       "\n",
       "                  bear_bull  \\\n",
       "0      {'basic': 'Bullish'}   \n",
       "1      {'basic': 'Bullish'}   \n",
       "2      {'basic': 'Bullish'}   \n",
       "3      {'basic': 'Bullish'}   \n",
       "4      {'basic': 'Bullish'}   \n",
       "...                     ...   \n",
       "99995                   NaN   \n",
       "99996  {'basic': 'Bullish'}   \n",
       "99997                   NaN   \n",
       "99998                   NaN   \n",
       "99999  {'basic': 'Bearish'}   \n",
       "\n",
       "                                                    text      tickers  \\\n",
       "0                               $GME ridiculous √∞¬ü¬í¬µ√∞¬ü¬ò¬Ç          GME   \n",
       "1                                    $GME wooooooooooooo          GME   \n",
       "2      $GME make your MONEY BACK with $GAXY; she&#39;...     GME|GAXY   \n",
       "3      $GME the fact that this is barred from trendyi...          GME   \n",
       "4      $BB $GME Most discussed tickers on WSB; lets t...       BB|GME   \n",
       "...                                                  ...          ...   \n",
       "99995  $SPY LOL SUCH INTELLECTUALS √∞¬ü¬§¬£√∞¬ü¬§¬£√∞¬ü¬§¬£   CAN...  GME|SPY|AMC   \n",
       "99996                                               $GME          GME   \n",
       "99997  $FRX I would like to blame the dip on $GME... ...      FRX|GME   \n",
       "99998                   $GME to those who bought at $160          GME   \n",
       "99999                                              $GME           GME   \n",
       "\n",
       "                       timestamp        date  \n",
       "0      2021-01-27 17:45:09+00:00  2021-01-27  \n",
       "1      2021-01-26 20:14:29+00:00  2021-01-26  \n",
       "2      2021-02-09 16:41:06+00:00  2021-02-09  \n",
       "3      2021-03-05 17:14:06+00:00  2021-03-05  \n",
       "4      2021-01-22 19:08:05+00:00  2021-01-22  \n",
       "...                          ...         ...  \n",
       "99995  2021-02-18 18:10:52+00:00  2021-02-18  \n",
       "99996  2021-01-13 16:59:23+00:00  2021-01-13  \n",
       "99997  2021-02-24 20:45:46+00:00  2021-02-24  \n",
       "99998  2021-01-25 19:12:29+00:00  2021-01-25  \n",
       "99999  2021-02-09 18:00:53+00:00  2021-02-09  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitID</th>\n",
       "      <th>author</th>\n",
       "      <th>author_followers</th>\n",
       "      <th>author_ideas</th>\n",
       "      <th>author_join</th>\n",
       "      <th>bear_bull</th>\n",
       "      <th>text</th>\n",
       "      <th>tickers</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278552853</td>\n",
       "      <td>Investeren</td>\n",
       "      <td>54</td>\n",
       "      <td>3790</td>\n",
       "      <td>2012-10-22</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$GME ridiculous √∞¬ü¬í¬µ√∞¬ü¬ò¬Ç</td>\n",
       "      <td>GME</td>\n",
       "      <td>2021-01-27 17:45:09+00:00</td>\n",
       "      <td>2021-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>277914988</td>\n",
       "      <td>flappinAnolips</td>\n",
       "      <td>9</td>\n",
       "      <td>1554</td>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$GME wooooooooooooo</td>\n",
       "      <td>GME</td>\n",
       "      <td>2021-01-26 20:14:29+00:00</td>\n",
       "      <td>2021-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>286576565</td>\n",
       "      <td>jag2pr</td>\n",
       "      <td>10</td>\n",
       "      <td>287</td>\n",
       "      <td>2018-05-16</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$GME make your MONEY BACK with $GAXY; she&amp;#39;...</td>\n",
       "      <td>GME|GAXY</td>\n",
       "      <td>2021-02-09 16:41:06+00:00</td>\n",
       "      <td>2021-02-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>299825525</td>\n",
       "      <td>Tavrabbit</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>2020-12-08</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$GME the fact that this is barred from trendyi...</td>\n",
       "      <td>GME</td>\n",
       "      <td>2021-03-05 17:14:06+00:00</td>\n",
       "      <td>2021-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276427744</td>\n",
       "      <td>Pupperzzz</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$BB $GME Most discussed tickers on WSB; lets t...</td>\n",
       "      <td>BB|GME</td>\n",
       "      <td>2021-01-22 19:08:05+00:00</td>\n",
       "      <td>2021-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>291526781</td>\n",
       "      <td>PLelek</td>\n",
       "      <td>519</td>\n",
       "      <td>6407</td>\n",
       "      <td>2016-05-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$SPY LOL SUCH INTELLECTUALS √∞¬ü¬§¬£√∞¬ü¬§¬£√∞¬ü¬§¬£   CAN...</td>\n",
       "      <td>GME|SPY|AMC</td>\n",
       "      <td>2021-02-18 18:10:52+00:00</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>272968454</td>\n",
       "      <td>kluski</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>2016-03-11</td>\n",
       "      <td>{'basic': 'Bullish'}</td>\n",
       "      <td>$GME</td>\n",
       "      <td>GME</td>\n",
       "      <td>2021-01-13 16:59:23+00:00</td>\n",
       "      <td>2021-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>294751108</td>\n",
       "      <td>Cheze</td>\n",
       "      <td>13</td>\n",
       "      <td>703</td>\n",
       "      <td>2020-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$FRX I would like to blame the dip on $GME... ...</td>\n",
       "      <td>FRX|GME</td>\n",
       "      <td>2021-02-24 20:45:46+00:00</td>\n",
       "      <td>2021-02-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>277303198</td>\n",
       "      <td>PhatPuffDaddy</td>\n",
       "      <td>198</td>\n",
       "      <td>14351</td>\n",
       "      <td>2019-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$GME to those who bought at $160</td>\n",
       "      <td>GME</td>\n",
       "      <td>2021-01-25 19:12:29+00:00</td>\n",
       "      <td>2021-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>286646208</td>\n",
       "      <td>3monkeys</td>\n",
       "      <td>12</td>\n",
       "      <td>2362</td>\n",
       "      <td>2020-11-23</td>\n",
       "      <td>{'basic': 'Bearish'}</td>\n",
       "      <td>$GME</td>\n",
       "      <td>GME</td>\n",
       "      <td>2021-02-09 18:00:53+00:00</td>\n",
       "      <td>2021-02-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows √ó 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is somewhat formatted, but there is an encoding issue we need to fix (see the odd characters). We'll use `ftfy` to do that, but we'll do this as part of a larger cleaning and data-prep process.\n",
    "\n",
    "Specifically, we will use `gensim`'s implementation of Word2Vec. Unlike `sklearn`, `gensim` requires textual data to be tokenized in a list of lists, where the outer element is the level we wish to analyze (e.g., sentence, paragraph, document) and inner element is token. \n",
    "\n",
    "We're examining tweets here, so we are going to use a special tokenizer in `nltk` to help handle these tweets. We'll incorporate this into a function that:\n",
    "1. Addresses the formatting issue in the tweets\n",
    "2. Accounts for cash tags (e.g., $MSFT)\n",
    "3. Removes stop-words \n",
    "4. Retains only emojis, cash tags, or letter-based tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b25bd6736c5d5171"
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import ftfy, emoji, re\n",
    "# Initialize an empty list to store each row as an entry\n",
    "stops = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open('/Users/cooperdenning/PycharmProjects/pythonProject5/.venv/D6.2/english', 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Strip the newline character and append the line to the list\n",
    "        stops.append(line.strip())\n",
    "\n",
    "# Now, lines_list contains each line of the file as an element\n",
    "print(stops)\n",
    "\n",
    "\n",
    "tweettoken = TweetTokenizer()\n",
    "\n",
    "print(stops)\n",
    "cashtag = re.compile(r'\\$(?=[a-z]+)') # this is a complex reg-ex that uses a look ahead.\n",
    "\n",
    "def myTweetTokenizer(tweet):\n",
    "    tweet = ftfy.ftfy(str(tweet)).lower() # 1. fix encoding issue and convert to lower-case (str accounts for occassional missing value)\n",
    "    tweet = cashtag.sub('CT',tweet) # 2. replaces cash tag with \"CT\"\n",
    "    tokens = [t for t in tweettoken.tokenize(tweet) if (emoji.is_emoji(t) or (len(t)>=2 and t.isalpha() and t not in stops))] # steps 3-4\n",
    "    return tokens\n",
    "\n",
    "formatted_tweets = df['text'].apply(myTweetTokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:39:16.059138Z",
     "start_time": "2024-11-11T19:39:03.334094Z"
    }
   },
   "id": "dbae82891daf92e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "formatted_tweets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:39:19.670566Z",
     "start_time": "2024-11-11T19:39:19.666602Z"
    }
   },
   "id": "d9784404424b92e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                [CTgme, ridiculous, üíµ, üòÇ]\n",
       "1                                  [CTgme, wooooooooooooo]\n",
       "2        [CTgme, make, money, back, CTgaxy, increase, n...\n",
       "3        [CTgme, fact, barred, trendying, mass, media, ...\n",
       "4        [CTbb, CTgme, discussed, tickers, wsb, lets, t...\n",
       "                               ...                        \n",
       "99995    [CTspy, lol, intellectuals, ü§£, ü§£, ü§£, cant, eve...\n",
       "99996                                              [CTgme]\n",
       "99997    [CTfrx, would, like, blame, dip, CTgme, starte...\n",
       "99998                                      [CTgme, bought]\n",
       "99999                                              [CTgme]\n",
       "Name: text, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Train Word2Vec Model\n",
    "Next, we'll train the __[Word2Vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)__ model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb91d78b46e8c73e"
  },
  {
   "cell_type": "code",
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=formatted_tweets,\n",
    "                 workers=4, # parallelize across 4 cores\n",
    "                 sg=0, # CBOW (Skim-gram would be sg=1)\n",
    "                 min_count=10, # require a word to appear at least 10 times\n",
    "                 vector_size=100 # the dimension of the vector space we project to\n",
    "                ) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:43:42.035883Z",
     "start_time": "2024-11-11T19:43:37.546617Z"
    }
   },
   "id": "8ab6dc2d7a1c06a8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "deAnd that's it! The model is trained. \n",
    "\n",
    "## Step 3: \n",
    "Now, let's examine some word similarities. `gensim` has several very intuitive methods built in. For instance, suppose we wanted to know the 10 most similar words to the word \"revenue\":"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5f8ea5180c99ad1"
  },
  {
   "cell_type": "code",
   "source": [
    "model.wv.most_similar('revenue',topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:44:03.276706Z",
     "start_time": "2024-11-11T19:44:03.198932Z"
    }
   },
   "id": "503c33aae3f06ef2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sales', 0.9355709552764893),\n",
       " ('net', 0.8825573325157166),\n",
       " ('growth', 0.8759857416152954),\n",
       " ('dividend', 0.8747082352638245),\n",
       " ('recent', 0.8744640946388245),\n",
       " ('valued', 0.8717338442802429),\n",
       " ('eps', 0.8643818497657776),\n",
       " ('global', 0.8636304140090942),\n",
       " ('sector', 0.8629682660102844),\n",
       " ('announced', 0.859205961227417)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "model.wv.most_similar('wsb',topn=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:44:07.284669Z",
     "start_time": "2024-11-11T19:44:07.225309Z"
    }
   },
   "id": "2879b126fdf99478",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reddit', 0.8106210827827454),\n",
       " ('pumping', 0.6959242820739746),\n",
       " ('main', 0.6904810667037964),\n",
       " ('calling', 0.677744448184967),\n",
       " ('crowd', 0.6752024292945862),\n",
       " ('wallstreet', 0.6580294966697693),\n",
       " ('kids', 0.6521521210670471),\n",
       " ('focus', 0.6454545855522156),\n",
       " ('pumped', 0.6412028074264526),\n",
       " ('army', 0.6331405639648438)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "You could also look at some of the language typical in financial social media:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eba407174504dadb"
  },
  {
   "cell_type": "code",
   "source": [
    "for w in ['üíé','yolo','üöÄ','üôå']:\n",
    "    print(f\"{w}:\")\n",
    "    print(model.wv.most_similar(w,topn=10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T19:44:11.636636Z",
     "start_time": "2024-11-11T19:44:11.580048Z"
    }
   },
   "id": "4cc822816dd88ea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíé:\n",
      "[('üôå', 0.8508074283599854), ('üß§', 0.840610146522522), ('üñê', 0.8269316554069519), ('‚úã', 0.8266353011131287), ('ü§ö', 0.8264672160148621), ('ü§≤', 0.823300302028656), ('üôåüèº', 0.8201403021812439), ('üëê', 0.8188973069190979), ('‚úäüèª', 0.798251748085022), ('ü•ú', 0.7785016298294067)]\n",
      "yolo:\n",
      "[('lotto', 0.8439286947250366), ('outs', 0.8281283974647522), ('printing', 0.8260573744773865), ('stockorbit', 0.8084323406219482), ('leap', 0.7973029613494873), ('commons', 0.7946054935455322), ('shoulda', 0.7911477088928223), ('delayed', 0.788819432258606), ('assigned', 0.7838578820228577), ('itm', 0.7756495475769043)]\n",
      "üöÄ:\n",
      "[('üåù', 0.800750195980072), ('üåô', 0.8003227710723877), ('ü™ê', 0.7703695297241211), ('üåö', 0.7700921297073364), ('üåï', 0.7454323768615723), ('üí™üèª', 0.7054610848426819), ('ü§üüèº', 0.6954057216644287), ('CTentx', 0.6929192543029785), ('üë®\\u200düöÄ', 0.6911655068397522), ('moon', 0.6909932494163513)]\n",
      "üôå:\n",
      "[('üëê', 0.8907890319824219), ('‚úã', 0.8850992918014526), ('üñê', 0.882362425327301), ('üß§', 0.873512864112854), ('üôåüèº', 0.8617876768112183), ('ü§ö', 0.8582518100738525), ('üíé', 0.8508073091506958), ('ü§≤', 0.8496029376983643), ('‚úäüèª', 0.8331674337387085), ('ü§≤üèΩ', 0.8316590785980225)]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also look at the similarity between any two words in the corpus:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61191d276ed5d7eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.wv.similarity('income','eps')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96800c44c8b59a7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The word vectors are accessible in the `model.wv.vectors` attribute. These are indexed by the vocabulary, and we can link back to that index with `model.wv.index_to_key`. I think it's easiest to access this information with a dataframe:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdeb865f97f427f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w2v_vectors = pd.DataFrame(model.wv.vectors,index=model.wv.index_to_key)\n",
    "w2v_vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "959d6c29bb29fe5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, suppose we wanted to represent our corpus of tweets as the sum of individual word vectors. We could do this in two different ways. If we want to allow scale to influence distances, we could sum the vectors. Alternatively, we can use the mean to remove the effect of scale. Here's a function that can handle both (just change `aggfunc` to `np.sum` if you would like to sum instead of compute the mean):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab91c0c8eb0da057"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_tweet(tweet,aggfunc = np.mean):\n",
    "    vectors = np.array([w2v_vectors.loc[word] for word in tweet if word in w2v_vectors.index])\n",
    "    return pd.Series(aggfunc(vectors,axis=0)).explode()\n",
    "\n",
    "# Test it out:\n",
    "vectorize_tweet(formatted_tweets.iloc[0])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b62631f43fbe4036"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now run on full corpus:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6fc315f56ecbd86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tw_vectors = formatted_tweets.apply(lambda x: vectorize_tweet(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e4557b77b9da3c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tw_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "60beb0b82c0ab69a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
