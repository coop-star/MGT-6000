{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4 - Earnings Announcement Language\n",
    "You’ve taken a new position at a boutique brokerage firm that wishes to focus on a “value-based” investment strategy. The firm has several analysts devoted to analyzing the extent to which quantitative financial statement information predicts future returns. You’ve been asked to consider qualitative information, particularly the narrative information accompanying firms’ earnings announcements.\n",
    "\n",
    "Your supervisor has provided you with a sample of 2,000 earnings announcements, which data scientists (err… a PhD student) at the firm have already cleaned and parsed. You’ve also been provided with abnormal market returns surrounding each firm’s earnings announcement.\n",
    "\n",
    "The firm would like to understand which words appear to be most predictive of the immediate investor reaction to earnings news. You have been asked to identify words conveying both “positive tone” (words that strongly predict positive returns) and “negative tone” (words that strongly predict negative returns). You remember from your applied analytics class that two finance professors, Tim Loughran and Bill McDonald, maintain a finance sentiment dictionary. You think it’d be worthwhile to consider financial sentiment from this dictionary as well.\n",
    "\n",
    "Your task involves the following requirements:\n",
    "1.\tConstruct a case insensitive document-term matrix using all language from the earnings announcements. Your matrix should only include the 2,000 most common words (assuming they meet the criteria below), and you should allow for bigrams *and* trigrams. You should exclude:\n",
    "\n",
    "    a)\tstop words\n",
    "    \n",
    "    b)\tany non-alpha tokens\n",
    "    \n",
    "    c)\ttokens shorter than 3 characters in length\n",
    "    \n",
    "    d)\ttokens appearing in more than 90% of earnings announcements\n",
    "\n",
    "    Report the top 25 words in your matrix.\n",
    "\n",
    "2. Using the Loughran and McDonald financial sentiment dictionary, compute overall sentiment using three measures:\n",
    "\n",
    "    a) positive words / total words\n",
    "   \n",
    "    b) negative words / total words\n",
    "   \n",
    "    c) (positive words – negative words) / total words\n",
    "\n",
    "    Report descriptive statistics for these three measures (i.e., `.describe().transpose()`).\n",
    "\n",
    "3. Compute the correlations between each word in your matrix. Answer these questions:\n",
    "\n",
    "    a) How do each of these three measures correlate with returns?\n",
    "   \n",
    "    b) Are you surprised by this pattern? Why or why not?\n",
    "\n",
    "4. Scale each row of the document term matrix by the total words in the document (i.e., so that the “counts” are proportions and sum to 1). Using these percentages, list the 25 words that correlate most positively with returns and most negatively with returns (50 total words). Report how many of these appear in the financial sentiment dictionary used in question 2.\n",
    "\n",
    "5. (6046 only) Fit an LDA model using `sklearn` with 50 topics. Identify the topic that correlates most positively and most negative with returns. For these two topics, summarize the most relevant 10 words for each, and whether you find this pattern intuitive.\n",
    "\n",
    "### Requirement 1\n",
    "In this step, we will first load the data, and then generate the document term matrix needed to answer the specific requirements.\n",
    "\n",
    "First, load the data and briefly inspect. There are two data files:\n",
    "- `Task4_ea_sample.zip`: sample of earnings announcements\n",
    "- `EA_list.csv`: Returns data for each earnings announcement\n",
    "\n",
    "We haven't done much with zip files, so I will provide the code to load this data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "\n",
    "folder = '/Users/cooperdenning/Documents/GitHub/MGT-6000/Task4' # Update with your own path\n",
    "archive = f'{folder}/Task4_ea_sample.zip'\n",
    "\n",
    "eadata = []\n",
    "with ZipFile(archive,'r') as arc:\n",
    "    for mem in arc.namelist(): #namelist() lists files in zip archive, so this loop iterates over those\n",
    "        if mem.endswith('txt'):  # if the member is a txt file, it's extracted and added to the \"eadata\" list\n",
    "            contents = arc.read(mem)\n",
    "            eadata.append([mem,contents])\n",
    "\n",
    "textdf = pd.DataFrame(eadata,columns=['File_Name','text'])\n",
    "textdf['text'] = textdf['text'].str.decode('utf8')\n",
    "textdf['File_Name'] = textdf['File_Name'].str.split(\"/\").str[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:21.824860Z",
     "start_time": "2024-11-18T16:38:21.459991Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now load the 'ea_list.csv' data into a dataframe called `ealist` and merge with `textdf` using the field `File_Name`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ealist = pd.read_csv(f'{folder}/EA_list.csv')\n",
    "both = pd.merge(ealist, textdf, how='inner',on='File_Name')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:23.157948Z",
     "start_time": "2024-11-18T16:38:23.148733Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Report `info()` and `head(10)` for both:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# print info() and head(10) for both datasets\n",
    "\n",
    "both.info()\n",
    "both.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:24.381977Z",
     "start_time": "2024-11-18T16:38:24.370884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   File_Name      2000 non-null   object \n",
      " 1   CIK            2000 non-null   int64  \n",
      " 2   datadate       2000 non-null   object \n",
      " 3   announce_date  2000 non-null   object \n",
      " 4   tic            2000 non-null   object \n",
      " 5   AbnReturn      2000 non-null   float64\n",
      " 6   text           2000 non-null   object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 109.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                          File_Name      CIK    datadate announce_date   tic  \\\n",
       "0   790705-0000950123-10-100486.txt   790705   9/30/2010    2010-11-04  TKLC   \n",
       "1   814547-0001144204-14-003352.txt   814547  12/31/2013    2014-01-22  FICO   \n",
       "2    71829-0001437749-14-019134.txt    71829   9/30/2014    2014-10-30    NR   \n",
       "3    71691-0001157523-19-000236.txt    71691  12/31/2018    2019-02-06   NYT   \n",
       "4   320193-0001193125-10-230992.txt   320193   9/30/2010    2010-10-18  AAPL   \n",
       "5  1666134-0001171843-18-005691.txt  1666134   6/30/2018    2018-08-02    BL   \n",
       "6    29332-0000029332-11-000064.txt    29332   6/30/2011    2011-08-04  DXYN   \n",
       "7  1318605-0001564590-20-033069.txt  1318605   6/30/2020    2020-07-22  TSLA   \n",
       "8   731012-0000950123-11-010074.txt   731012  12/31/2010    2011-02-07  HCSG   \n",
       "9  1282637-0001193125-11-103679.txt  1282637   3/31/2011    2011-04-20   NEU   \n",
       "\n",
       "   AbnReturn                                               text  \n",
       "0  -1.826796  EX-99. 1 2 v57726exv99w1. htm EX-99. 1  exv99w...  \n",
       "1  -3.337049  EX-99. 1 2 v365966_ex99-1. htm EXHIBIT 99. 1  ...  \n",
       "2   4.051841  EX-99 2 ex99-1. htm EXHIBIT 99. 1  ex99-1. htm...  \n",
       "3  16.480962  EX-99. 1 2 a51935890ex99_1. htm EXHIBIT 99. 1 ...  \n",
       "4  -1.228231  EX-99. 1 2 dex991. htm TEXT OF PRESS RELEASE I...  \n",
       "5  10.491547  EX-99. 1 2 exh_991. htm PRESS RELEASE  EdgarFi...  \n",
       "6  13.407912  EX-99. 1 2 ex99_1pressrel-01. htm 2Q  2011 PRE...  \n",
       "7  -9.107826  EX-99. 1 2 tsla-ex991_63. htm EX-99. 1   tsla-...  \n",
       "8   4.447709  EX-99. 1 2 c11979exv99w1. htm EXHIBIT 99. 1  E...  \n",
       "9  13.225461  EX-99. 1 2 dex991. htm PRESS RELEASE REGARDING...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>CIK</th>\n",
       "      <th>datadate</th>\n",
       "      <th>announce_date</th>\n",
       "      <th>tic</th>\n",
       "      <th>AbnReturn</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>790705-0000950123-10-100486.txt</td>\n",
       "      <td>790705</td>\n",
       "      <td>9/30/2010</td>\n",
       "      <td>2010-11-04</td>\n",
       "      <td>TKLC</td>\n",
       "      <td>-1.826796</td>\n",
       "      <td>EX-99. 1 2 v57726exv99w1. htm EX-99. 1  exv99w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814547-0001144204-14-003352.txt</td>\n",
       "      <td>814547</td>\n",
       "      <td>12/31/2013</td>\n",
       "      <td>2014-01-22</td>\n",
       "      <td>FICO</td>\n",
       "      <td>-3.337049</td>\n",
       "      <td>EX-99. 1 2 v365966_ex99-1. htm EXHIBIT 99. 1  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71829-0001437749-14-019134.txt</td>\n",
       "      <td>71829</td>\n",
       "      <td>9/30/2014</td>\n",
       "      <td>2014-10-30</td>\n",
       "      <td>NR</td>\n",
       "      <td>4.051841</td>\n",
       "      <td>EX-99 2 ex99-1. htm EXHIBIT 99. 1  ex99-1. htm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71691-0001157523-19-000236.txt</td>\n",
       "      <td>71691</td>\n",
       "      <td>12/31/2018</td>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>NYT</td>\n",
       "      <td>16.480962</td>\n",
       "      <td>EX-99. 1 2 a51935890ex99_1. htm EXHIBIT 99. 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320193-0001193125-10-230992.txt</td>\n",
       "      <td>320193</td>\n",
       "      <td>9/30/2010</td>\n",
       "      <td>2010-10-18</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-1.228231</td>\n",
       "      <td>EX-99. 1 2 dex991. htm TEXT OF PRESS RELEASE I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1666134-0001171843-18-005691.txt</td>\n",
       "      <td>1666134</td>\n",
       "      <td>6/30/2018</td>\n",
       "      <td>2018-08-02</td>\n",
       "      <td>BL</td>\n",
       "      <td>10.491547</td>\n",
       "      <td>EX-99. 1 2 exh_991. htm PRESS RELEASE  EdgarFi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29332-0000029332-11-000064.txt</td>\n",
       "      <td>29332</td>\n",
       "      <td>6/30/2011</td>\n",
       "      <td>2011-08-04</td>\n",
       "      <td>DXYN</td>\n",
       "      <td>13.407912</td>\n",
       "      <td>EX-99. 1 2 ex99_1pressrel-01. htm 2Q  2011 PRE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1318605-0001564590-20-033069.txt</td>\n",
       "      <td>1318605</td>\n",
       "      <td>6/30/2020</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>-9.107826</td>\n",
       "      <td>EX-99. 1 2 tsla-ex991_63. htm EX-99. 1   tsla-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>731012-0000950123-11-010074.txt</td>\n",
       "      <td>731012</td>\n",
       "      <td>12/31/2010</td>\n",
       "      <td>2011-02-07</td>\n",
       "      <td>HCSG</td>\n",
       "      <td>4.447709</td>\n",
       "      <td>EX-99. 1 2 c11979exv99w1. htm EXHIBIT 99. 1  E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1282637-0001193125-11-103679.txt</td>\n",
       "      <td>1282637</td>\n",
       "      <td>3/31/2011</td>\n",
       "      <td>2011-04-20</td>\n",
       "      <td>NEU</td>\n",
       "      <td>13.225461</td>\n",
       "      <td>EX-99. 1 2 dex991. htm PRESS RELEASE REGARDING...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, construct a document term matrix using `CountVectorizer` using the specifications provided above. Your matrix should exclude:\n",
    "\n",
    "a)\tstop words (for simplicity, you can set `stop_words = 'english')`\n",
    "\n",
    "b)\tany non-alpha tokens\n",
    "\n",
    "c)\ttokens shorter than 3 characters in length\n",
    "\n",
    "d)\ttokens appearing in more than 90% of earnings announcements\n",
    "\n",
    "Also, limit the DTM to 2,000 words, bigrams, or trigrams, and do not consider case sensitivity.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stops = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(f'{folder}/english', 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Strip the newline character and append the line to the list\n",
    "        stops.append(line.strip())\n",
    "\n",
    "# Now, lines_list contains each line of the file as an element\n",
    "\n",
    "vec =  CountVectorizer(\n",
    "    stop_words='english',             # Exclude stop words\n",
    "    token_pattern=r'\\b[a-zA-Z]{3,}\\b',  # Only include alpha tokens with 3+ chars\n",
    "    max_df=0.9,                       # Exclude tokens in more than 90% of docs\n",
    "    ngram_range=(1, 3),               # Include unigrams, bigrams, and trigrams\n",
    "    max_features=2000,                # Limit vocabulary to 2000 features\n",
    "    lowercase=True                    # Case insensitive processing\n",
    ")\n",
    "dtm = vec.fit_transform(both['text'])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:39.029558Z",
     "start_time": "2024-11-18T16:38:26.724198Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Report the top 25 words in your matrix (BONUS POINT: Report the top 25 words AND the count of each)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "vocab = vec.vocabulary_\n",
    "\n",
    "# Get feature names (columns in the resulting DataFrame)\n",
    "feature_names = vec.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to dense if needed and create DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=feature_names)\n",
    "# Insert code to generate top 25 word counts (and for bonus point, count of each)\n",
    "vocab_df = pd.DataFrame(list(vocab.items()), columns=[\"Word\", \"Index\"])\n",
    "vocab_df['Count'] = [dtm_df.iloc[:, idx].sum() for idx in vocab_df['Index']]\n",
    "\n",
    "# Sort by count and select the top 25 words\n",
    "top_25_vocab_df = vocab_df.sort_values(by='Count', ascending=False).head(25)[['Word', 'Count']]\n",
    "\n",
    "top_25_vocab_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:41.134608Z",
     "start_time": "2024-11-18T16:38:41.044013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 Word  Count\n",
       "8                gaap  46793\n",
       "1055         adjusted  30494\n",
       "353          non gaap  28293\n",
       "203             sales  27446\n",
       "38               loss  25300\n",
       "383        net income  24976\n",
       "2             revenue  23094\n",
       "388      months ended  18163\n",
       "1570           ebitda  17013\n",
       "33           measures  15944\n",
       "286        activities  12955\n",
       "262          december  12860\n",
       "73             second  12163\n",
       "95           revenues  12163\n",
       "104      amortization  11885\n",
       "4           increased  11846\n",
       "219              debt  11705\n",
       "98            percent  11579\n",
       "1157           fourth  11427\n",
       "829           segment  11148\n",
       "1273   fourth quarter  11127\n",
       "395    second quarter  10944\n",
       "10             margin  10672\n",
       "108            impact  10616\n",
       "1642  adjusted ebitda  10548"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gaap</td>\n",
       "      <td>46793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>adjusted</td>\n",
       "      <td>30494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>non gaap</td>\n",
       "      <td>28293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>sales</td>\n",
       "      <td>27446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>loss</td>\n",
       "      <td>25300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>net income</td>\n",
       "      <td>24976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>revenue</td>\n",
       "      <td>23094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>months ended</td>\n",
       "      <td>18163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>ebitda</td>\n",
       "      <td>17013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>measures</td>\n",
       "      <td>15944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>activities</td>\n",
       "      <td>12955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>december</td>\n",
       "      <td>12860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>second</td>\n",
       "      <td>12163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>revenues</td>\n",
       "      <td>12163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>amortization</td>\n",
       "      <td>11885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>increased</td>\n",
       "      <td>11846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>debt</td>\n",
       "      <td>11705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>percent</td>\n",
       "      <td>11579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>fourth</td>\n",
       "      <td>11427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>segment</td>\n",
       "      <td>11148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>fourth quarter</td>\n",
       "      <td>11127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>second quarter</td>\n",
       "      <td>10944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>margin</td>\n",
       "      <td>10672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>impact</td>\n",
       "      <td>10616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>adjusted ebitda</td>\n",
       "      <td>10548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement 2\n",
    "\n",
    "Using the Loughran and McDonald financial sentiment dictionary, compute overall sentiment using three measures:\n",
    "\n",
    "a) positive words / total words\n",
    "\n",
    "b) negative words / total words\n",
    "\n",
    "c) (positive words – negative words) / total words\n",
    "    \n",
    "First, load the LM Dictionary (file name is `LoughranMcDonald_SentimentWordLists_2018.xlsx`. Recall that the positive and negative terms are in separate sheets. Once you've loaded the language, store the positive and negative words in a list. I recommend converting to lower case at this stage.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lmdict = f\"{folder}/LoughranMcDonald_SentimentWordLists_2018.xlsx\"\n",
    "lmpos = pd.read_excel(lmdict,sheet_name='Positive',header=None,names=['word'])\n",
    "lmneg = pd.read_excel(lmdict,sheet_name='Negative',header=None,names=['word'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:45.853658Z",
     "start_time": "2024-11-18T16:38:45.755485Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# save lower case version of each word in lmpos and lmneg in two sets\n",
    "pos = set(lmpos['word'].str.lower())  # Convert to lowercase and store in a set\n",
    "neg = set(lmneg['word'].str.lower()) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:46.400502Z",
     "start_time": "2024-11-18T16:38:46.395643Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second, identify the indices in the vocabulary that correspond to positive and negative words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Identify indices for positive and negative words in the vocabulary\n",
    "posidx = [v for k, v in vec.vocabulary_.items() if k in pos]\n",
    "negidx = [v for k, v in vec.vocabulary_.items() if k in neg]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:47.766658Z",
     "start_time": "2024-11-18T16:38:47.758812Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "Third, compute the required sentiment measures and add them to the combined dataframe, `both`. Specifically add these three measures to the dataframe:\n",
    "\n",
    "a) `pos_pct` = positive words / total words\n",
    "\n",
    "b) `neg_pct` = negative words / total words\n",
    "\n",
    "c) `net_pos` = (positive words – negative words) / total words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "poswords = np.asarray(dtm[:,posidx].sum(axis=1)).flatten()\n",
    "negwords = np.asarray(dtm[:,negidx].sum(axis=1)).flatten()\n",
    "totwords = np.asarray(dtm.sum(axis=1)).flatten()\n",
    "\n",
    "# Compute the sentiment measures\n",
    "both['pos_pct'] = poswords / totwords\n",
    "both['neg_pct'] = negwords / totwords\n",
    "both['net_pos'] = (poswords - negwords) / totwords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:49.769252Z",
     "start_time": "2024-11-18T16:38:49.760165Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "both[['pos_pct', 'neg_pct', 'net_pos']].describe().transpose()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:38:50.832034Z",
     "start_time": "2024-11-18T16:38:50.822061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          count      mean       std       min       25%       50%       75%  \\\n",
       "pos_pct  2000.0  0.012726  0.005795  0.000583  0.008749  0.011907  0.015798   \n",
       "neg_pct  2000.0  0.017444  0.010692  0.000000  0.009298  0.015707  0.023848   \n",
       "net_pos  2000.0 -0.004718  0.011520 -0.049675 -0.011567 -0.003674  0.002980   \n",
       "\n",
       "              max  \n",
       "pos_pct  0.053219  \n",
       "neg_pct  0.063321  \n",
       "net_pos  0.041064  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos_pct</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.012726</td>\n",
       "      <td>0.005795</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.011907</td>\n",
       "      <td>0.015798</td>\n",
       "      <td>0.053219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_pct</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.017444</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009298</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>0.023848</td>\n",
       "      <td>0.063321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_pos</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>-0.004718</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>-0.049675</td>\n",
       "      <td>-0.011567</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.041064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement 3\n",
    "Answer these questions:\n",
    "\n",
    "a) How do each of these three measures correlate with returns?\n",
    "b) Are you surprised by this pattern? Why or why not?\n",
    "\n",
    "To answer 3A, simply generate a correlation matrix with the variables needed to assess the correlation between returns and sentiment:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Generating the correlation matrix between returns (`AbnReturn`) and sentiment measures\n",
    "correlation_matrix = both[['AbnReturn', 'pos_pct', 'neg_pct', 'net_pos']].corr()\n",
    "\n",
    "# Displaying the correlation matrix\n",
    "correlation_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:39:07.229495Z",
     "start_time": "2024-11-18T16:39:07.222902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           AbnReturn   pos_pct   neg_pct   net_pos\n",
       "AbnReturn   1.000000  0.036189 -0.048489  0.063213\n",
       "pos_pct     0.036189  1.000000  0.122707  0.389189\n",
       "neg_pct    -0.048489  0.122707  1.000000 -0.866441\n",
       "net_pos     0.063213  0.389189 -0.866441  1.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AbnReturn</th>\n",
       "      <th>pos_pct</th>\n",
       "      <th>neg_pct</th>\n",
       "      <th>net_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AbnReturn</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036189</td>\n",
       "      <td>-0.048489</td>\n",
       "      <td>0.063213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_pct</th>\n",
       "      <td>0.036189</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.122707</td>\n",
       "      <td>0.389189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_pct</th>\n",
       "      <td>-0.048489</td>\n",
       "      <td>0.122707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_pos</th>\n",
       "      <td>0.063213</td>\n",
       "      <td>0.389189</td>\n",
       "      <td>-0.866441</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer to 3B:**\n",
    "Not really, the strongest relationship is between net_pos and neg_pct (-0.866), which is logical due to how the measures are constructed. \n",
    "\n",
    "Returns are influenced by a variety of factors, including macroeconomic conditions, geopolitical environment, and media portrayal. Textual sentiment is only one piece of the puzzle and may not always have a strong predictive relationship with returns.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement 4\n",
    "We'll now look at how you might start to construct your own dictionary. First, scale each row of the document term matrix by the total words in the document (i.e., so that the “counts” are proportions and sum to 1)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dtm_scaled = dtm / totwords[:, np.newaxis]  # Broadcasting to scale rows\n",
    "\n",
    "# Convert to dense array\n",
    "dtm_scaled = dtm_scaled.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:39:19.605041Z",
     "start_time": "2024-11-18T16:39:19.574239Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these percentages, generate correlations between each word count and returns. List the 25 words that correlate most positively with returns and most negatively with returns (50 total words). Report how many of these appear in the financial sentiment dictionary used in requirement 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dtm_ret = np.hstack([both['AbnReturn'].values.reshape(-1,1),dtm_scaled])\n",
    "corrs = np.corrcoef(dtm_ret,rowvar=False)\n",
    "\n",
    "# Compute correlations between each word proportion and AbnReturn\n",
    "# Extract AbnReturn as the first column\n",
    "returns = dtm_ret[:, 0]\n",
    "\n",
    "# Compute correlations of each word (column) with returns\n",
    "word_correlations = np.corrcoef(dtm_ret, rowvar=False)[0, 1:]\n",
    "\n",
    "# Get the top 25 positively and negatively correlated words\n",
    "sorted_indices = np.argsort(word_correlations)  # Sort correlations\n",
    "\n",
    "# 25 most positively correlated\n",
    "top_positive_indices = sorted_indices[-25:]\n",
    "top_positive_words = [(vec.get_feature_names_out()[i], word_correlations[i]) for i in top_positive_indices]\n",
    "\n",
    "# 25 most negatively correlated\n",
    "top_negative_indices = sorted_indices[:25]\n",
    "top_negative_words = [(vec.get_feature_names_out()[i], word_correlations[i]) for i in top_negative_indices]\n",
    "\n",
    "# Combine into a single list\n",
    "top_words = top_positive_words + top_negative_words\n",
    "\n",
    "# Check how many of these words appear in the financial sentiment dictionary\n",
    "lm_dict_words = pos.union(neg)  # Combine positive and negative dictionary words\n",
    "dictionary_matches = [word for word, _ in top_words if word in lm_dict_words]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:39:22.052183Z",
     "start_time": "2024-11-18T16:39:21.196263Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "List the 25 words that correlate most positively with returns and most negatively with returns (50 total words) (BONUS POINT FOR REPORTING WORD + CORRELATION). "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ret_corrs = corrs[1:,0].flatten() # Grabs the first column, skipping upper left element\n",
    "print(\"Most Negative Words\")\n",
    "top_negative_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:39:23.180778Z",
     "start_time": "2024-11-18T16:39:23.176187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Negative Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('net loss', -0.10571868850311193),\n",
       " ('net loss income', -0.08571840188975391),\n",
       " ('site', -0.07773283069185974),\n",
       " ('loss income', -0.07617007189789415),\n",
       " ('rent', -0.0754569426245722),\n",
       " ('conjunction', -0.07446446307367027),\n",
       " ('employee', -0.07209179265624727),\n",
       " ('included', -0.07172953177691292),\n",
       " ('operating loss', -0.07152646753189354),\n",
       " ('start', -0.06893282079350574),\n",
       " ('publicly', -0.06871491761308089),\n",
       " ('million compared', -0.06813421957379075),\n",
       " ('contribution', -0.06759510723427935),\n",
       " ('gaap gross profit', -0.06743012623080652),\n",
       " ('loss', -0.06536294149176902),\n",
       " ('ceo', -0.06493621954878402),\n",
       " ('enhance', -0.0630490538981664),\n",
       " ('million compared million', -0.06179089723417775),\n",
       " ('nasdaq', -0.06174798298469522),\n",
       " ('involve', -0.061089418249149846),\n",
       " ('described', -0.05914426064073346),\n",
       " ('net current', -0.057358000615496396),\n",
       " ('write', -0.0566766768137),\n",
       " ('parties', -0.055859749558654796),\n",
       " ('fiscal year', -0.05577877769395761)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Most Positive Words\")\n",
    "top_positive_words.reverse()\n",
    "top_positive_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-18T16:43:12.507398Z",
     "start_time": "2024-11-18T16:43:12.502822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Positive Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('excluding', 0.06925928128556731),\n",
       " ('diluted eps', 0.06779897060473143),\n",
       " ('free', 0.06484286019833677),\n",
       " ('record', 0.06453959230372946),\n",
       " ('different', 0.06346869395055169),\n",
       " ('free cash flow', 0.06279697673379618),\n",
       " ('net income', 0.06266065652395256),\n",
       " ('statements income', 0.06206529744318952),\n",
       " ('free cash', 0.06078639603148409),\n",
       " ('consolidated statements income', 0.06077706488256738),\n",
       " ('ended june june', 0.06005425468430923),\n",
       " ('cash flow', 0.059511539739231824),\n",
       " ('june june', 0.05941922033512571),\n",
       " ('average shares', 0.05804716965880158),\n",
       " ('option', 0.05759084276838067),\n",
       " ('weighted average shares', 0.05506712723626777),\n",
       " ('rate', 0.05476673957949416),\n",
       " ('selling general administrative', 0.054684764255307064),\n",
       " ('strong', 0.05436175411637299),\n",
       " ('flow', 0.053569348989459926),\n",
       " ('diluted net income', 0.05351204849585309),\n",
       " ('risks associated', 0.053482787218680494),\n",
       " ('achieved', 0.05309693361785794),\n",
       " ('expenses current assets', 0.05278734438315649),\n",
       " ('divestiture', 0.05241970104056571)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, report how many of these appear in the financial sentiment dictionary used in requirement 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create sets of the top correlated words\n",
    "top_positive_set = set(word for word, _ in top_positive_words)\n",
    "top_negative_set = set(word for word, _ in top_negative_words)\n",
    "\n",
    "# Find overlaps with the financial sentiment dictionary\n",
    "negative_overlap = top_negative_set.intersection(neg)  # Overlapping negative words\n",
    "positive_overlap = top_positive_set.intersection(pos)  # Overlapping positive words\n",
    "\n",
    "# Print the counts of overlapping words\n",
    "print(f\"There are {len(negative_overlap)} overlapping words for negative\")\n",
    "print(f\"There are {len(positive_overlap)} overlapping words for positive\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T21:42:34.041735Z",
     "start_time": "2024-11-16T21:42:34.035746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 overlapping words for negative\n",
      "There are 2 overlapping words for positive\n",
      "{'strong', 'achieved'}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Requirement 5 (6046 only)\n",
    "\n",
    "Fit an LDA model using `sklearn` with 50 topics. Set the `random_state` to 123, and leave everything else at default settings. You can use the `dtm` you generated earlier. I recomment using `fit_transform()` so you have the topic matrix ready to analyze correlations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "lda = LDA(n_components=50, random_state=123)  # Create LDA model with 50 topics\n",
    "\n",
    "# Fit the LDA model to the data and transform the document-term matrix to the topic matrix\n",
    "topics = lda.fit_transform(dtm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T22:26:35.284991Z",
     "start_time": "2024-11-16T22:24:56.145848Z"
    }
   },
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, generate correlations between `AbnReturn` and topic weights for each of the 2,000 earnings press releases (HINT: This will be very similar to the prior step, except you do not need to scale the topics matrix by total words)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert AbnReturn Series to a NumPy array and reshape\n",
    "AbnReturn = both['AbnReturn'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Combine AbnReturn with the topic weights\n",
    "top_ret = np.hstack([AbnReturn, topics])  # Shape: (2000, 51)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corrs_top = np.corrcoef(top_ret, rowvar=False)  # Correlation matrix of shape (51, 51)\n",
    "# Extract correlations of AbnReturn with topic weights\n",
    "ret_top_corrs = corrs_top[0, 1:]  # First row, excluding the self-correlation\n",
    "ret_top_corrs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-17T01:07:34.944059Z",
     "start_time": "2024-11-17T01:07:34.939686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05708245e-05 1.05708245e-05 1.05708245e-05 4.11041950e-01\n",
      " 1.05708245e-05 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 1.05708245e-05 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 1.05708245e-05 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 1.05708245e-05 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 1.05708245e-05 2.14176997e-02 3.39668085e-02 6.43049400e-02\n",
      " 1.05708245e-05 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 1.05708245e-05 1.05708245e-05 3.78399891e-02 1.05708245e-05\n",
      " 1.05708245e-05 1.74268611e-01 1.05708245e-05 1.35799554e-01\n",
      " 1.05708245e-05 1.54677941e-02 3.13227425e-02 1.05708245e-05\n",
      " 1.05708245e-05 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 7.43133048e-03 1.05708245e-05 1.05708245e-05 1.05708245e-05\n",
      " 4.09661402e-02 2.57707489e-02]\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, identify the topic that correlates most positively and most negatively with returns. Report the 10 words most relevant to that topics. I've provided a function you can use to access those words."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the topic-word matrix\n",
    "top_word = lda.components_  # Shape: (50, 2000)\n",
    "\n",
    "# Verify vocab alignment\n",
    "\n",
    "# Define function to get top words for a topic\n",
    "def get_topic_words(topic, top_word, vocab, topn=10):\n",
    "    top_words = top_word[topic, :].argsort()[-topn:][::-1].tolist()  # Top word indices\n",
    "    return [vocab[i] for i in top_words]  # Map indices to vocab\n",
    "\n",
    "# Find the most positively and negatively correlated topics\n",
    "mostpos = np.argmax(ret_top_corrs)  # Topic index with highest positive correlation\n",
    "mostneg = np.argmin(ret_top_corrs)  # Topic index with highest negative correlation\n",
    "\n",
    "# Convert vocab dictionary to a sorted list\n",
    "vocab_list = [word for word, index in sorted(vocab.items(), key=lambda item: item[1])]\n",
    "\n",
    "\n",
    "# Retrieve the top 10 words for each topic\n",
    "pos_words = get_topic_words(mostpos, top_word, vocab_list, topn=10)\n",
    "neg_words = get_topic_words(mostneg, top_word, vocab_list, topn=10)\n",
    "\n",
    "print(\"Most positively correlated topic:\", mostpos)\n",
    "\n",
    "print(\"Most negatively correlated topic:\", mostneg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T22:30:54.807099Z",
     "start_time": "2024-11-16T22:30:54.799705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most positively correlated topic: 11\n",
      "Most negatively correlated topic: 44\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Words for most positive topic:\")\n",
    "print(f\"{'|'.join(get_topic_words(mostpos,lda.components_,vocab_list,topn=10))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T22:30:56.637770Z",
     "start_time": "2024-11-16T22:30:56.635328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for most positive topic:\n",
      "continuing|continuing operations|loss|gaap|discontinued|income continuing|income continuing operations|discontinued operations|net income|income loss\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Words for most negative topic:\")\n",
    "print(f\"{'|'.join(get_topic_words(mostneg,lda.components_,vocab_list,topn=10))}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T22:30:57.155826Z",
     "start_time": "2024-11-16T22:30:57.153569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for most negative topic:\n",
      "loss|net loss|loss income|adjusted|sales|net loss income|impairment|gaap|million quarter|primarily\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Final Question:** \n",
    "Do you find the topic pattern intuitive? In your opinion, does the topic-based approach appear to better identify themes that should correlate with returns?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
