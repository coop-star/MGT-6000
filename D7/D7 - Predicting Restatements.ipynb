{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo 7\n",
    "In this demonstration, we will learn to build ML-based classifiers to predict the likelihood of a restatement. I've provided you with a small set of financial-statement based variables that were derived based on the \"FSCORE\" model developed in __[Dechow, Ge, Larson, and Sloan (2011)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1911-3846.2010.01041.x)__. The FSCORE continues be used as a measure of reporting risk. The original version is based on a logistic regression. We'll see if we can improve upon that with some ML-based models.\n",
    "\n",
    "We will proceed in the following steps:\n",
    "1. Load and inspect the data. Standardize all variables to have mean 0 and standard deviation 1. Then, fit a simple logistic regression and evaluate the quality of the out-of-sample model fit.\n",
    "2. Evaluate the out-of-sample performance of a Decision Tree and Random Forest using default parameters. Evaluate impact of varying tree depth in the random forest.\n",
    "3. Tune the Decision Tree using `RandomizedSearchCV`. Repeat tuning with a custom evaluation function that more heavily penalizes false negatives than false positives.\n",
    "4. Employ resampling and evaluate whether this improves model performance.\n",
    "\n",
    "### Step 1: Load & Standardize the data\n",
    "Let's first load the data. The data is saved in an \"HDF\" format, which is a highly portable data format which retains data-types (i.e., the `datetime` objects will not need to be reformatted). Note that to load this data, you'll need to install the dependency `pytables` (`!pip install pytables` should work)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder = \".\"\n",
    "df = pd.read_hdf(f\"{folder}/fscore.h5\",key='key')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 12 total columns in the data. Recall that `gvkey` is a firm identifier, and `datadate` is the fiscal year-end. The next 7 variables are the predictors we'll use (our X) variables. The final three variables are all different specifications of restatements:\n",
    "\n",
    "* `BigR`: A material misstatement requiring the company to issue a press-release announcing the problem\n",
    "* `AnyR`: Any restatement; includes `BigR` as well as \"quiet\" restatements which are simply adjusted in future financial filings\n",
    "* `Intentional`: Restatements that are deemed intentional misrepresentations, as evidenced by SEC investigations\n",
    "\n",
    "For the purpose of this demonstration, we'll attempt to predict `AnyR`, but let's look at the descriptive statistics:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[['BigR','AnyR','Intentional']].describe().transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we're going to standardize our data. Recall that certain machine-learning models require this, and others are more flexible. Generally, though, standardization never hurts. It retains all information in the original data but allows comparability across factors.\n",
    "\n",
    "To standardize data to have mean 0, standard deviation 1, we can employ `sklearn`'s `StandardScaler`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean=True,with_std=True) # these are defaults, but says to adjust by mean and SD\n",
    "\n",
    "XVars = ['RSSTaccruals','ChangeReceivables','ChangeInventory','PctSoftAssets','ChCashSales','chROA','ActualIssuance']\n",
    "\n",
    "stdX = scaler.fit_transform(df[XVars])\n",
    "\n",
    "stdX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output of standard scaler is a numpy array; let's get these back in a dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stdX_df = pd.DataFrame(stdX,columns=XVars,index=df.index)\n",
    "stdX_df.describe().transpose() # confirm mean 0, std 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's estimate a logistic regression so we have a benchmark to compare to. For all evaluations, we'll use a hold out sample. `sklearn` includes a convenient function called `train_test_split` that is used to conduct these splits. However, we're going to be a little careful with how we conduct this split.\n",
    "\n",
    "When extracting a holdout sample, it's imperative that the holdout sample be *independent* of the data used to fit the model. In our case, we have multiple years per firm in our data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.groupby('gvkey')['AnyR'].count().value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Specifically, we have between 1 and 14 years of data for each unique `gvkey`. Do you think MSFT in 2010 looks similar to MSFT in 2011? Of course! So, having MSFT 2010 in the training data and MSFT 2011 in the validation data would violate this independence.\n",
    "\n",
    "Therefore, we're going to split our sample of `gvkey`s into training and testing datasets. We'll do this in two steps. First, we'll identify the gvkeys in each group. Then, we'll identify the indices that correspond to each set. This will simplify things later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_gvkeys,test_gvkeys = train_test_split(df['gvkey'].unique(),test_size=0.20,random_state=123)\n",
    "print(f\"There are {len(train_gvkeys)} companies in the training data, and {len(test_gvkeys)} companies in the testing data.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, identify the dataframe indices for each of these groups:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_idx = df[df['gvkey'].isin(train_gvkeys)].index\n",
    "test_idx  = df[df['gvkey'].isin(test_gvkeys)].index\n",
    "print(f\"There are {len(train_idx)} observations in the training data, and {len(test_idx)} observations in the testing data.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's see if the two datasets look comparable:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stdX_df.loc[train_idx].describe().transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stdX_df.loc[test_idx].describe().transpose()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not identical, but pretty similar. How about restatement rates?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[train_idx,'AnyR'].mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[test_idx,'AnyR'].mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also pretty similar!\n",
    "\n",
    "Finally, let's fit our logistic regression. For simplicity, we're going to set up four datasets that we can use in the future (will save a lot of typing): `Xtrain`, `Xtest`, `ytrain`, `ytest`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Xtrain = stdX_df.loc[train_idx]\n",
    "Xtest  = stdX_df.loc[test_idx]\n",
    "\n",
    "ytrain = df.loc[train_idx,'AnyR']\n",
    "ytest  = df.loc[test_idx,'AnyR']\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(Xtrain,ytrain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've now fit our model. `sklearn` has a variety of __[evaluation metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)__ available for us to use. We're going to look at a `confusion_matrix` and a `classification_report`. Let's start with *in-sample* performance. Note that these metrics (almost) universally accept `ytrue` first, and then \"`ypred`\", or the predicted Y from the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "confusion_matrix(ytrain,logit.predict(Xtrain))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not very good! What does the classification report (a report that breaks down precision recall, and f1-scores by class) look like?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(ytrain,logit.predict(Xtrain)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What do you make of this output? Is the model biased? Accurate?\n",
    "\n",
    "Given the poor performance in-sample, out-of-sample will likely be equally poor:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(ytest,logit.predict(Xtest)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Fit a Decision Tree and Random Forest\n",
    "Next, we're going to evaluate the performance of a __[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)__ and a __[RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)__.\n",
    "\n",
    "Each of these models has numerous parameters, but we'll start with the defaults:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "dtree = DT(random_state=123)\n",
    "rf = RF(random_state=123)\n",
    "\n",
    "dtree.fit(Xtrain,ytrain)\n",
    "rf.fit(Xtrain,ytrain)\n",
    "\n",
    "print(\"In-sample performance of Decision Tree:\\n\")\n",
    "print(classification_report(ytrain,dtree.predict(Xtrain)))\n",
    "\n",
    "print(\"In-sample performance of Random Forest:\\n\")\n",
    "print(classification_report(ytrain,rf.predict(Xtrain)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow! Let's check out-of-sample:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"In-sample performance of Decision Tree:\\n\")\n",
    "print(classification_report(ytest,dtree.predict(Xtest)))\n",
    "\n",
    "print(\"In-sample performance of Random Forest:\\n\")\n",
    "print(classification_report(ytest,rf.predict(Xtest)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What happened?\n",
    "\n",
    "The model was clearly **overfit**. This happens because we didn't set a maximum tree depth. We can see how deep the tree is with the `get_depth()` method:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtree.get_depth()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the random forest, I can compute the average depth of each tree in the forest:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean([tree.get_depth() for tree in rf.estimators_])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is far too high. To illustrate, let's take our random forest and iterate over max depth from 1 to 50, counting by 2s, and we'll collect the f1-score for both in- and out-of-sample evaluations. Give the class imbalance, I'm going to go ahead and introduce some class weights here, but we'll come back to this later:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "diag = []\n",
    "for depth in range(1,51):\n",
    "    print(depth)\n",
    "    record = {'max_depth':depth}\n",
    "    rf = RF(max_depth=depth,random_state=123,n_jobs=-1,class_weight={0:0.1,1:0.9})\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    record['f1_in-sample'] = f1_score(ytrain,rf.predict(Xtrain),average='macro')\n",
    "    record['f1_out-of-sample'] = f1_score(ytest,rf.predict(Xtest),average='macro')\n",
    "    diag.append(record)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll plot these measures."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfdiag = pd.DataFrame(diag)\n",
    "rfdiag"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rfdiag['max_depth'], rfdiag['f1_in-sample'], label='In-sample F1', linestyle='-')\n",
    "plt.plot(rfdiag['max_depth'], rfdiag['f1_out-of-sample'], label='Out-of-Sample F1', linestyle='--')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'd find the same thing if we did this with a Decision Tree.\n",
    "\n",
    "We can fix this issue with Cross-validation. Cross-validation is relatively easy to implement. Let's run one instance, and then we'll repeat our loop above to show how it resolves the issue:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "rf = RF(max_depth=25,random_state=123)\n",
    "cv_results = cross_validate(rf,Xtrain,ytrain,cv=5,scoring='f1_macro',return_estimator=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The F1-scores for the 5 hold out folds are in `cv_results['test_score']`. We can compute the mean of these to evaluate performance:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(cv_results['test_score'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see how this average compares to the validation sample, we can use the 5 different fitted models to generate predictions for our validation data, and take the average of those scores:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_of_sample_cv = [f1_score(ytest,est.predict(Xtest),average='macro') for est in cv_results['estimator']]\n",
    "np.mean(out_of_sample_cv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Tune the model\n",
    "Now, we're going to tune our model. To save time, we're going to tune the decision tree, but the exact same principles would apply if you were tuning a random forest (you'd just have different parameters to tune).\n",
    "\n",
    "`sklearn` offers two different classes for tuning models.\n",
    "\n",
    "* __[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)__ performs a grid search over all possible parameter combinations that you pass it. The benefit of this approach is that it's comprehensive. The downside is it can take forever if you choose a large set of parameters.\n",
    "* __[RandomizedSeachCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)__ is very similar to the grid search, except that it randomly chooses combinations. Often, this does a good job of finding a good set of hyperparameters. We'll use the randomized search.\n",
    "\n",
    "To use these methods, we must specify a set of parameters to try. For this, it's best to look at the documenation for the model you are tuning. Here's the link to the __[DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)__ again. You can also view the parameters for any model using the `get_params()` method:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = DT()\n",
    "df.get_params()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our purposes, we'll select a few different hyperparameters and provide some options. We do this with a dictionary, where the key corresponds to the hyperparameter and the value is a list of potential outcomes (or something else that can return a random value):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtparams = {'ccp_alpha':[0,0.001,0.01],\n",
    "            'class_weight':[None,'balanced',{0:0.05,1:1},{0:1,1:2}],\n",
    "            'max_depth':[2,5,10,20,30],\n",
    "            'criterion':['gini','entropy','log_loss'],\n",
    "            'max_features':[None,'log2','sqrt'],\n",
    "            'random_state':[123]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we did a grid search, this would produce 540 different combinations (if my math is right). The randomized search will randomly sample various sets. Note that we include `'random_state'` as an input but don't vary it. I don't typically tune the random state, though in some scenarios that may be appropriate.\n",
    "\n",
    "`RandomizedSearchCV` is very easy to use. It essentially acts as a trainable object. It's also highly customizable. We'll incorporate cross-validation (actually the default) and try to optimize the macro F1-score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV as RS\n",
    "\n",
    "# first initialize an empty Decision Tree\n",
    "dtree = DT()\n",
    "\n",
    "# Now set up the randomized search. Note that we'll optimize on 'f1', which focuses only on the \"1s\" (restatements)\n",
    "rs = RS(dtree,param_distributions=dtparams,n_iter=50,scoring='f1',n_jobs=-1,cv=5,verbose=4,random_state=123)\n",
    "results = rs.fit(Xtrain,ytrain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access results, we can look at `rs.cv_results_`, which is a dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rs.cv_results_.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `'mean_test_score'` will tell us the F1 score for the validation folds:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rs.cv_results_['mean_test_score']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can convert this dictionary to a dataframe, which makes it easier to inspect:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(rs.cv_results_)\n",
    "results_df.sort_values(by='mean_test_score',ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also quickly look at the \"best parameters\" with the `best_params_` attribute:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rs.best_params_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can evaluate how this best model works on our hold out sample, treating it as if it were a standard `sklearn` object. Let's look at a confusion matrix:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = confusion_matrix(ytest,rs.predict(Xtest))\n",
    "c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One way to evaluate performance is to look at how much more likely you are to correctly identify a restatement in the right column than left:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(c[1,1]/c[:,1].sum()) / (c[1,0]/c[:,0].sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose this is the metric we want to maximize. We can incorporate this into our randomized search using a custom scorer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def myscorer(ytrue,ypred): # these must be the two arguments\n",
    "    return (c[1,1]/c[:,1].sum()) / (c[1,0]/c[:,0].sum())\n",
    "\n",
    "scorer = make_scorer(myscorer,greater_is_better=True)\n",
    "\n",
    "rs = RS(dtree,param_distributions=dtparams,n_iter=50,scoring=scorer,n_jobs=-1,cv=5,verbose=4,random_state=123)\n",
    "results2 = rs.fit(Xtrain,ytrain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Did we pick a model that performs better on this metric?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results2.best_score_)\n",
    "print(results2.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performance was the same, though the \"best\" parameters were slightly different. I think this is because what we came up with was essentially a transformation of the original metric (though one that's a little easier to understand perhaps).\n",
    "\n",
    "In any case, we could define something more involved. Suppose we want to score with something like this, which may be how a regulator or auditor would view this problem:\n",
    "\n",
    "* 20 points for every correctly identified restatement (c[1,1])\n",
    "* -20 points for every missed restatement (c[1,0])\n",
    "* -1 point for every falsely identified restatement (c[0,1])\n",
    "* 2 points for every correctly identified non restatement (c[0,0])\n",
    "\n",
    "We'll then scale that score by the count of observations:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def myscorer2(ytrue,ypred): # these must be the two arguments\n",
    "    c = confusion_matrix(ytrue,ypred)\n",
    "    score = 20*(c[1,1] - c[1,0]) - c[0,1] + 2*c[0,0]\n",
    "    return score/c.sum()\n",
    "\n",
    "scorer2 = make_scorer(myscorer2,greater_is_better=True)\n",
    "\n",
    "rs = RS(dtree,param_distributions=dtparams,n_iter=50,scoring=scorer2,n_jobs=-1,cv=5,verbose=4,random_state=123)\n",
    "results2 = rs.fit(Xtrain,ytrain)\n",
    "print(results2.best_score_)\n",
    "print(results2.best_params_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at this confusion matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_matrix(ytest,rs.predict(Xtest))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall is pretty good in this model (506 / (257+506)), but precision is bad (506 vs 5641+506).\n",
    "\n",
    "Let's finish up by looking at resampling to see if that can help improve things."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Resampling\n",
    "\n",
    "The last thing we will cover is resampling. Note that when resampling, you **must only resample** the training data. Your model needs to be evaluated in terms of how it performs on *real* data.\n",
    "\n",
    "We will incorporate resampling into our parameter search in a moment, but before doing so let's quickly look at how it works. `sklearn` does not natively include resamplers, but there is a separate package, `imblearn`, that is designed to work like `sklearn`. You'll need to install if you haven't already (`!pip install imblearn`).\n",
    "\n",
    "We'll import an oversampler and undersampler:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE # SMOTE creates synthetic data\n",
    "\n",
    "rus = RandomUnderSampler(random_state=123)\n",
    "smote = SMOTE(random_state=123,n_jobs=-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with undersampling. We'll first undersample our training data (both X and y). We'll then train a `DecisionTreeClassifier`, and evaluate performance on the hold out sample:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xus,yus = rus.fit_resample(Xtrain,ytrain)\n",
    "print(Xtrain.shape)\n",
    "print(Xus.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's fit our original decision tree again so we have it for comparison purposes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtree = DT(random_state=123)\n",
    "dtree.fit(Xtrain,ytrain)\n",
    "print(classification_report(ytest,dtree.predict(Xtest)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we'll use the resampled data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtree.fit(Xus,yus)\n",
    "print(classification_report(ytest,dtree.predict(Xtest)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By most metrics, overall performance declines. BUT, the model is less biased (particularly on recall). That's the idea with resampling.\n",
    "\n",
    "Let's now check out oversampling:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Xos,yos = smote.fit_resample(Xtrain,ytrain)\n",
    "print(Xtrain.shape)\n",
    "print(Xos.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice the dataset grew because we added some synthetic data. Let's repeat our training:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dtree.fit(Xos,yos)\n",
    "print(classification_report(ytest,dtree.predict(Xtest)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is less biased than the original version, though more biased than undersampling (at least on recall).\n",
    "\n",
    "As a final step, let's think about how we'd incorporate this into parameter search. Specifically, how do we deal with resampling with cross-validation, since we only want to resample the folds used for training.\n",
    "\n",
    "The answer is that we're going to need to set up something called a **pipeline** to handle a set of instructions, which are applied, in order, for each step set of parameters we try. Fortunately, `imblearn` has set something up that makes this relatively easy to implement. We need to make two adjustments:\n",
    "\n",
    "1. Set up a pipeline that we can tune with `RandomizedSearchCV`\n",
    "2. Adjust our parameter dictionary to account for multiple steps in the pipeline (i.e. I may have parameters for both the classifier AND the resampler)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "### Set Up the Pipeline ###\n",
    "# The pipeline is a list of tuples, and each tuple as a name (e.g., 'smote'), and object (e.g., SMOTE())\n",
    "pipeline = Pipeline([\n",
    "    ('smote',SMOTE()),\n",
    "    ('dtree',DT())\n",
    "])\n",
    "\n",
    "### Set up the parameter grid ###\n",
    "# We'll start with the one we used before, but we need to prepend each key with 'dtree__'\n",
    "params2 = {f'dtree__{k}':v for k,v in dtparams.items()}\n",
    "# Now, add some parameters for smote:\n",
    "params2['smote__random_state'] = [123]\n",
    "params2['smote__k_neighbors'] = [2,3,4,5]\n",
    "\n",
    "params2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can set up a randomized search like before, but use the pipeline instead of the decision tree:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compared to above, change to pipeline and params2\n",
    "rs = RS(pipeline,param_distributions=params2,n_iter=50,scoring=scorer2,n_jobs=-1,cv=5,verbose=4,random_state=123)\n",
    "rs.fit(Xtrain,ytrain)\n",
    "print(classification_report(ytest,rs.predict(Xtest)))\n",
    "print(confusion_matrix(ytest,rs.predict(Xtest)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that's it! We've now learned to:\n",
    "\n",
    "1. Implement basic ML models and generate a variety of performance reports\n",
    "2. Tune these models\n",
    "3. Incorporate resampling into our tuning process\n",
    "\n",
    "While we mainly focused on decision trees, the **exact** same approach applies to essentially all __[`sklearn` classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)__. One just needs to adjust hyperparameters to match those available for tuning in the object."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
