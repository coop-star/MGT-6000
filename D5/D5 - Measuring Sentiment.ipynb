{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Demo 5 - Measuring Sentiment\n",
    "In this demonstration, we'll learn how to construct a measure of sentiment from a larger corpus. Note that we are starting with cleaned textual data so we can focus on the actual measurement of sentiment, but cleaning the data is a vital step in any NLP setting.\n",
    "\n",
    "To measure sentiment, we'll first rely on two different sentiment dictionaries. The first is a basic, general purpose dictionary and the second is a dictionary developed specifically for financial sentiment. We'll conclude with a pattern-based method that is available through a package called `textblob`. We'll go through these steps:\n",
    "\n",
    "1. Load the dataset and generate a document-term matrix with `sklearn`.\n",
    "2. Measure sentiment using the Harvard General Inquirer Dictionary\n",
    "3. Measure sentiment using the Loughran-McDonald Dictionary\n",
    "4. Measure sentiment using `textlob`\n",
    "5. Evaluate correlations among measures\n",
    "\n",
    "### Step 1: Load and Setup Data\n",
    "I've provided you with a small set of MD&A disclosures from a random sample of 10-K filings. For ease of use, these are in a compressed csv file, so we can easily load directly with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:31:33.006730Z",
     "start_time": "2023-10-23T14:31:32.241647Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder = \"./\"\n",
    "df = pd.read_csv(\"./MDAs201.csv.gz\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's look at one specific disclosure so we can get a sense of general structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:32:50.981640Z",
     "start_time": "2023-10-23T14:32:50.966063Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(df.iloc[0]['mda'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "These tend to be fairly long disclosures, suggesting we'll have a large volume of text we can use for our analyses.\n",
    "\n",
    "To generate the document-term matrix, we will use `sklearn`'s __[`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)__. Note that one can generate TF-IDF weighted counts with `TfidfVectorizer`. It works the same exact way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:55:05.708116Z",
     "start_time": "2023-10-23T14:55:04.156362Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_features=1500,min_df=5)\n",
    "dtm = vec.fit_transform(df['mda'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "I left parameters as defaults for most things. The two I updated, `max_features` and `min_df` controls the volume and sparsity of words. We can inspect the shape of the `dtm` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:56:59.605381Z",
     "start_time": "2023-10-23T14:56:59.599306Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We have 141 rows (# of documents) and 1500 unique words. If we want to access the vocabulary, we can use the `vocabulary_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:57:48.533681Z",
     "start_time": "2023-10-23T14:57:48.528039Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This vocabulary allows us to find the index of certain words, but often it's more useful to go the other way. We could flip this dictionary (`{v:k for k,v in vec.vocabulary_.items()}`), but I usually just grab the vocabulary in `np.array` form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T14:59:29.015686Z",
     "start_time": "2023-10-23T14:59:29.009919Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vocab = vec.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This highlights that we have a lot of numbers here. Numbers won't tell us much in terms of qualitative sentiment, so let's restrict our vocabulary to words only. We can do this with a `token_pattern`, which is an attribute in the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:13:51.971824Z",
     "start_time": "2023-10-23T15:13:51.278399Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features=1500,min_df=5,token_pattern=r'[a-z]{2,}') # requires two or more letters. Note that the vectorizer defaults to lower case so we can leave out capital letters\n",
    "dtm = vec.fit_transform(df['mda'])\n",
    "vocab = vec.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This looks better. Now, let's look at which words are most common in our corpus. To do so, recall that we can sum *by column*, or `axis=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:13:54.262098Z",
     "start_time": "2023-10-23T15:13:54.258933Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dtm.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "How do we figure out which word has the highest word count? We need to first convert the sparse `matrix` into a numpy array, and we can look at some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:27:26.792807Z",
     "start_time": "2023-10-23T15:27:26.784843Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wcs = np.asarray(dtm.sum(axis=0)).flatten()\n",
    "wcs.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "One word appears almost 80,000 times! What word is it? We can use a method called __[`argmax()`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)__ to figure that out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:28:24.083555Z",
     "start_time": "2023-10-23T15:28:24.069137Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "wcs.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Which word is 1360?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:28:58.407993Z",
     "start_time": "2023-10-23T15:28:57.902332Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vocab[wcs.argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We did not address **stopwords**. Do we have a lot of stop words in our data? We can use `argsort()`, which behaves similar to `argmax()` to figure this out. For instance, suppose we wanted to look at the 10 most frequently occurring words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:30:36.359313Z",
     "start_time": "2023-10-23T15:30:36.352568Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vocab[wcs.argsort()[-10:][::-1]] # the [::-1] selects all, start from last to first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Fortunately it's easy to remove stop words. We could use `sklearn`'s list of stop words, but their own documentation recommends using one from `nltk`, so we'll do that. Let's load these stop words and then re-vectorize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:32:13.423342Z",
     "start_time": "2023-10-23T15:32:12.922668Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Notice that the stop words include contractions. We'll adjust our token pattern to allow those to match as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:34:50.958165Z",
     "start_time": "2023-10-23T15:34:50.199111Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features=1500,min_df=5,token_pattern=r'[a-z]\\'?[a-z]+',stop_words=stops) #\n",
    "dtm = vec.fit_transform(df['mda'])\n",
    "vocab = vec.get_feature_names_out()\n",
    "wcs = np.asarray(dtm.sum(axis=0)).flatten()\n",
    "vocab[wcs.argsort()[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Much better! Let's make one more adjustment to help deal with some of these other very common words. We could explicitly identify them and add to our list. Or we can set some threshold. Let's say we only want words that appear in fewer than 50% of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:36:12.891941Z",
     "start_time": "2023-10-23T15:36:12.264248Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features=1500,min_df=5,token_pattern=r'[a-z]\\'?[a-z]+',stop_words=stops,max_df=0.50) #\n",
    "dtm = vec.fit_transform(df['mda'])\n",
    "vocab = vec.get_feature_names_out()\n",
    "wcs = np.asarray(dtm.sum(axis=0)).flatten()\n",
    "vocab[wcs.argsort()[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 2: Measure sentiment using Harvard dictionary\n",
    "Let's go ahead and load the dictionary, which is in the file called \"inquirerbasic.xls\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:42:34.222745Z",
     "start_time": "2023-10-23T15:42:33.287226Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hvd = pd.read_excel(f\"{folder}/inquirerbasic.xls\")\n",
    "hvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "How many unique words are in the data? How about textual attributes?\n",
    "\n",
    "To grab words in a given list, we simply need to identify where a given column is not a `NaN`, and then keep the `Entry` for this. For instance, which words are deemed \"Hostile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:45:38.018240Z",
     "start_time": "2023-10-23T15:45:37.997470Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hvd.loc[hvd['Hostile'].notnull(),'Entry'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This short list illustrates one issue with the inquirer dictionary we'll need to address. Some words have multiple meanings or word forms. To address, we'll need to remove the \"#n\", where *n* is some number. We can do this with split, and we'll remove duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T15:47:27.062370Z",
     "start_time": "2023-10-23T15:47:27.041397Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hvd.loc[hvd['Hostile'].notnull(),'Entry'].str.lower().str.split(\"#\").str[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's go ahead and create our list of positive and negative words. We'll use `Positiv` and `Negativ`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:05:48.430320Z",
     "start_time": "2023-10-23T17:05:48.425611Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "positives = set(hvd.loc[hvd['Positiv'].notnull(),'Entry'].str.lower().str.split(\"#\").str[0].unique())\n",
    "negatives = set(hvd.loc[hvd['Negativ'].notnull(),'Entry'].str.lower().str.split(\"#\").str[0].unique())\n",
    "print(f\"There are {len(positives)} positive words and {len(negatives)} negative words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, we can very easily generate a measure of sentiment from our `dtm` by subsetting on the columns that appear in one of these lists. Recall we have the `dtm.vocabulary_` dictionary. We can use that to generate a list of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:10:33.421856Z",
     "start_time": "2023-10-23T17:10:33.414017Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "posidx = [v for k,v in vec.vocabulary_.items() if k in positives]\n",
    "negidx = [v for k,v in vec.vocabulary_.items() if k in negatives]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, we select the columns from `dtm`. Note that the way we access numpy objects is very similar to `loc`. We identify rows, then columns. In this case, we want all rows (`:`), and columns specified by `posidx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:11:59.412317Z",
     "start_time": "2023-10-23T17:11:59.405001Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dtm[:,posidx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To generate the count of positive words, we can sum. We need to do some `numpy` gymnastics to get things in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:18:24.854630Z",
     "start_time": "2023-10-23T17:18:24.848234Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.asarray(dtm[:,posidx].sum(axis=0)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can now directly add this to our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:20:18.979628Z",
     "start_time": "2023-10-23T17:20:18.959022Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['pos_words'] = np.asarray(dtm[:,posidx].sum(axis=1)).flatten()\n",
    "df['neg_words'] = np.asarray(dtm[:,negidx].sum(axis=1)).flatten()\n",
    "df['tot_words'] = np.asarray(dtm.sum(axis=1)).flatten()\n",
    "df[['pos_words','neg_words','tot_words']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Before we move on, I'll highlight some limitations of this approach:\n",
    "1. We did not consider alternative word formats. This diciontary is pretty comprehensive, but it is possible that there is a negative word of one format that doesn't appear with alternative formats (e.g., singular vs. plural). To address this, we could lemmatize our data, or use word stems (though this would be noisy).\n",
    "2. We do not consider *negation*, which can invert the sentiment of a given word. If you wished to consider negation, you would need to handle before you generated a document term matrix.\n",
    "3. We limited the size of our `dtm`, which could leave out some relatively rare words. In practice, this shouldn't have a large influence, but it's worth considering. For instance, let's compare the average positive word count here to one that uses all words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T17:28:16.681214Z",
     "start_time": "2023-10-23T17:28:16.012727Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vec2 = CountVectorizer(min_df=1,token_pattern=r'[a-z]\\'?[a-z]+',stop_words=stops,max_df=0.50) #\n",
    "dtm2 = vec2.fit_transform(df['mda'])\n",
    "vocab2 = vec2.get_feature_names_out()\n",
    "posidx2 = [v for k,v in vec2.vocabulary_.items() if k in positives]\n",
    "negidx2 = [v for k,v in vec2.vocabulary_.items() if k in negatives]\n",
    "np.asarray(dtm2[:,posidx2].sum(axis=1)).flatten().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 3: Financial Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To measure financial sentiment, we will use the Loughran and McDonald dictionary. The portion of the dictionary we will use was described in their __[2011 paper](https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2010.01625.x)__. The data is in the excel workbook I've provided, titled \"LoughranMcDonald_SentimentWordLists_2018.xlsx\".\n",
    "\n",
    "Note that in this workbook, each tab is a different set of words, and there are no headers. We can adjust our import to deal with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:32:05.373893Z",
     "start_time": "2023-10-23T19:32:04.996967Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lmpos = pd.read_excel(f'{folder}LoughranMcDonald_SentimentWordLists_2018.xlsx',\n",
    "                      sheet_name='Positive',header=None,names=['word'])\n",
    "lmpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:32:27.161221Z",
     "start_time": "2023-10-23T19:32:27.080379Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lmneg = pd.read_excel(f'{folder}LoughranMcDonald_SentimentWordLists_2018.xlsx',\n",
    "                      sheet_name='Negative',header=None,names=['word'])\n",
    "lmneg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, let's generate the same type of indices reference we did earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:36:34.378808Z",
     "start_time": "2023-10-23T19:36:34.376154Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lmposidx = [v for k,v in vec.vocabulary_.items() if k in lmpos['word'].values]\n",
    "lmnegidx = [v for k,v in vec.vocabulary_.items() if k in lmneg['word'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "How many positive words appear at least once in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:36:35.307980Z",
     "start_time": "2023-10-23T19:36:35.295124Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "len(lmposidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Zero?!\n",
    "\n",
    "We didn't adjust for case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:37:12.733355Z",
     "start_time": "2023-10-23T19:37:12.238830Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lmposidx = [v for k,v in vec.vocabulary_.items() if k in lmpos['word'].str.lower().values]\n",
    "lmnegidx = [v for k,v in vec.vocabulary_.items() if k in lmneg['word'].str.lower().values]\n",
    "len(lmposidx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now we can generate word counts for these two measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:42:24.049713Z",
     "start_time": "2023-10-23T19:42:24.037049Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['lmpos_words'] = np.asarray(dtm[:,lmposidx].sum(axis=1)).flatten()\n",
    "df['lmneg_words'] = np.asarray(dtm[:,lmnegidx].sum(axis=1)).flatten()\n",
    "df[['lmpos_words','lmneg_words']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 4: Pattern-based sentiment\n",
    "Our final measure of sentiment will be based on a pattern-based approach, which combines specific patterns of text with dictionary weights to generate sentiment. Note that these pattern-based approaches require the original text since they examine how words appear together.\n",
    "\n",
    "To generate these measures, we'll use `textblob`, a popular NLP package that allows access to an array of pre-built methods in a very simple API. I'll illustrate a few things on a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:45:13.734451Z",
     "start_time": "2023-10-23T19:45:13.726915Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "sample = df['mda'][0]\n",
    "blob = TextBlob(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We can easily look at words or sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:45:51.026342Z",
     "start_time": "2023-10-23T19:45:50.904490Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(blob.tokens)} tokens in this text.\")\n",
    "print(f\"There are {len(blob.sentences)} sentences in this text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "You can also identify specific attributes, like noun phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:46:40.168577Z",
     "start_time": "2023-10-23T19:46:31.618141Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "[np for np in blob.noun_phrases][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We are interested in *sentiment*, though. This is available as an attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:47:36.302492Z",
     "start_time": "2023-10-23T19:47:36.191553Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If we want polarity, we can access that directly or within \"sentiment\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:48:06.599640Z",
     "start_time": "2023-10-23T19:48:06.553416Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(blob.sentiment.polarity)\n",
    "print(blob.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Note that this polarity measure is meant to be applied to shorter spans of text, so I will usually generate polarity individually for all sentences in the data, and compute the average. Let's set up a function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:49:33.824884Z",
     "start_time": "2023-10-23T19:49:33.703188Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def measure_polarity(txt):\n",
    "    blob = TextBlob(txt)\n",
    "    return np.mean([s.polarity for s in blob.sentences])\n",
    "\n",
    "measure_polarity(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Finally, we can generate this measure as another column in our dataframe using `apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:51:20.864700Z",
     "start_time": "2023-10-23T19:51:08.656234Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['tb_polarity'] = df['mda'].apply(measure_polarity)\n",
    "df['tb_polarity'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5. Evaluate Correlations\n",
    "The final step in this demo is to evaluate the correlations among sentiment measures, and then we'll do one other exercise with correlations.\n",
    "\n",
    "The first step is relatively simple, but we first have to define our measures of general and financial sentiment. We'll use the formula (positive - negative) / total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:53:30.139943Z",
     "start_time": "2023-10-23T19:53:30.129762Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df['gen_sent'] = df.eval('(pos_words - neg_words)/tot_words')\n",
    "df['lm_sent'] = df.eval('(lmpos_words - lmneg_words)/tot_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now, we can examine overall correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T19:53:49.218179Z",
     "start_time": "2023-10-23T19:53:49.178315Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df[['gen_sent','lm_sent','tb_polarity']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "That's an interesting pattern. The sentiment based on the LM dictionary exhibits a *negative* correlation with the polarity-based sentiment. \n",
    "\n",
    "Before we conclude, let's see which words correlate most strongly with tb_polarity. We first need to generate a DTM that's deflated by the total word count (so each word is represented as a proportion of words), and then we'll examine correlations.\n",
    "\n",
    "Scaling the DV is relatively straightforward (with a little `numpy` manipulation; __[`reshape()`](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#numpy.reshape)__ is a complicated method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:01:46.355034Z",
     "start_time": "2023-10-23T20:01:46.348014Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dtm_scaled = np.asarray(dtm.todense()) / df['tot_words'].values.reshape(-1,1)\n",
    "dtm_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next, I'm going to correlate each column of this dtm with the measure of polarity in the dataframe. To do so, we'll combine the polarity with this dtm using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:37.191065Z",
     "start_time": "2023-10-23T20:36:37.172037Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dtm_pol = np.hstack([df['tb_polarity'].values.reshape(-1,1),dtm_scaled])\n",
    "# take care of missing values\n",
    "dtm_pol = np.nan_to_num(dtm_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Compute the correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:37.940277Z",
     "start_time": "2023-10-23T20:36:37.901641Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corrs = np.corrcoef(dtm_pol,rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:38.413548Z",
     "start_time": "2023-10-23T20:36:38.390879Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Now examine which words correlate most strongly. To do this, we can focus on the first column (or row) of this matrix. Let's use `argsort` on one column of data. We can then grab the top 5 and bottom 5 for our most negative and most positive correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:40.084483Z",
     "start_time": "2023-10-23T20:36:40.075112Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "top5 = corrs[0,1:].argsort()[-5:] # start at 1 to skip the top left corner\n",
    "top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:40.692266Z",
     "start_time": "2023-10-23T20:36:40.683698Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bottom5 = corrs[0,1:].argsort()[:5] # start at 1 to skip the top left corner\n",
    "bottom5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's now print each set of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:46.411995Z",
     "start_time": "2023-10-23T20:36:46.402400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"5 words with most positive correlation:\\n\")\n",
    "print(\"|\".join(vocab[top5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:47.853889Z",
     "start_time": "2023-10-23T20:36:47.831925Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"5 words with most negative correlation:\\n\")\n",
    "print(\"|\".join(vocab[bottom5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "If we want to see what these correlations are, we can access in original matrix, but this time we have to add 1 since we are referencing relative to index 0 (recall that we left out the first column of data above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:36:53.719063Z",
     "start_time": "2023-10-23T20:36:53.710416Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corrs[0,top5+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T20:37:10.666552Z",
     "start_time": "2023-10-23T20:37:10.663579Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "corrs[0,bottom5+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
